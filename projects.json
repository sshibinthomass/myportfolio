{
  "items": [
    {
      "sys": {
        "id": "1"
      },
      "fields": {
        "name": "OfficeGuardian",
        "category": "Robotics",
        "competition": "College Mini Project",
        "project": "Employee Management Bot",
        "description": "<p>\n    The COVID-19 pandemic has posed unprecedented challenges for small businesses around the world. To curb the\n    community transmission of the virus, governments have issued various standard operating procedures (SOPs) for\n    different sectors and industries. These SOPs may include measures such as wearing masks, maintaining social\n    distancing, checking body temperature, and recording attendance of employees and customers.<br><br>\n    However, implementing these SOPs can be costly and cumbersome for small businesses, especially if they rely on\n    manual or contact-based methods. For example, using a pen and paper or a fingerprint scanner to record attendance\n    can increase the risk of infection and cross-contamination. Moreover, manual methods can be prone to errors, fraud,\n    and manipulation, which can affect the accuracy and reliability of the data.<br><br>\n    Therefore, small businesses need an automated non-contact attendance system that can help them comply with the\n    COVID-19 SOPs in an efficient and affordable way. Such a system should be able to verify the identity of the people\n    who enter the premises, check their temperature and mask status, and record their attendance in a secure database.\n    The system should also be able to generate reports and alerts for the business owners or managers to monitor the\n    compliance level and take necessary actions.<br> <br>\n    In this project, we have developed a low-cost face recognition or barcode based internet of things (IoT)-enabled\n    COVID-19 SOP compliance system for small businesses.\n    <b>Our system consists of the following components:</b>\n<ul>\n    <li>When a person wants to enter the premises, he or she has to stand in front of the device and scan his or her\n        face or barcode. The face recognition or barcode recognition software will compare the scanned image with the\n        existing database and confirm if the person is authorized to enter or not.</li>\n    <li>If the person is authorized, the device will prompt him or her to remove his or her mask (if wearing one) and\n        measure his or her temperature using the infrared thermometer. The device will display the temperature reading\n        and indicate if it is within the normal range or not.</li>\n    <li>If the person has a normal temperature, he or she can put on his or her mask again (if required) and enter the\n        premises. The device will record his or her attendance along with other details such as name, time, date,\n        location, temperature, and mask status.</li>\n    <li>If the person has a high temperature or is not wearing a mask (if required), he or she will be denied entry and\n        asked to follow the appropriate SOPs. The device will also send an alert to the business owner or manager via\n        email or SMS.</li>\n    <li>The device will send all the data collected to the web application via the IoT platform. The web application\n        will store and process the data in a secure database.</li>\n    <li>The business owner or manager can log in to the web application and view and manage the data through a\n        dashboard. The dashboard will show various metrics such as total number of entries, number of entries with high\n        temperature, number of entries without mask, number of entries by time period, etc. The dashboard will also\n        generate reports and graphs that can help analyze the compliance level and identify any issues or trends.</li>\n\n</ul>\n<b>Our system has several advantages over other existing systems:</b>\n\n\n<ul>\n    <li>It is low-cost as it does not require any expensive hardware or installation. It can be configured using any\n        existing device with a camera and an internet connection.</li>\n    <li>It is non-contact as it does not require any physical contact between the person and the device or any other\n        equipment. It reduces the risk of infection and cross-contamination.</li>\n    <li>It is accurate as it uses advanced face recognition or barcode recognition technology that can identify people\n        even with changes in facial features or accessories. It also uses an infrared thermometer that can measure\n        temperature without touching.</li>\n    <li>It is flexible as it can be used with either face recognition or barcode recognition depending on the preference\n        and availability of resources. It can also be customized to suit different SOPs and scenarios.</li>\n    <li>It is user-friendly as it has a simple and intuitive interface that guides the user through the process. It also\n        has a voice assistant that can provide instructions and feedback in different languages.</li>\n    <li>It is scalable as it can handle multiple devices and locations. It can also be integrated with other systems\n        such as HRMS, payroll, or CRM.</li>\n</ul>\nWe believe that our system can help small businesses comply with the COVID-19 SOPs in an effective and affordable way.\nIt can also improve the safety and productivity of the employees and customers. We hope that our system can contribute\nto the fight against the pandemic and the recovery of the economy.\n</p>",
        "technology": "Deep Learning, Machine Learning, Robotics",
        "video": "https://www.youtube.com/embed/LkHWCjV7bn0",
        "image1": "OfficeGuardian1",
        "image2": "OfficeGuardian2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img14.png",
        "projectDate": "3 March 2022",
        "duration": "8 months",
        "github": "None",
        "App_link": "None"
      }
    },
    {
      "sys": {
        "id": "2"
      },
      "fields": {
        "name": "Spikora",
        "category": "App",
        "competition": "Rosary Plant House",
        "project": "A Prickly Paradise for Plant Lovers",
        "description": "<h2>RosaryPlantHouse.com – E-commerce Web Application</h2><p>  I developed <strong>RosaryPlantHouse.com</strong>, a static-first e-commerce  website specifically tailored for plant enthusiasts. This project represents a  complete end-to-end solution I created, encompassing everything from  architecture and data management to user experience and operational design.  The platform was designed for performance, simplicity, and minimal operational  overhead, and it showcases my skills in creative problem-solving, front-end  engineering, and strategic product thinking.</p><h3><strong>Executive Summary</strong></h3><p>  The Rosary Plant House application was engineered entirely as a static website  with client-side JavaScript powering all interactivity. This includes a  sophisticated product catalogue, cart management system, and a novel  WhatsApp-based checkout mechanism I designed as an innovative alternative to  traditional e-commerce infrastructure.</p><h3><strong>System Architecture and Technology Stack</strong></h3><h4><strong>'Static-First' E-Commerce Engine</strong></h4><p>  I built the site using a Jamstack approach, delivering pre-built HTML pages  such as <code>catalogue.html</code> and <code>restocked.html</code> for  performance and security. All cart, filtering, and UI logic is powered by  client-side JavaScript, removing the need for a backend server.</p><h4><strong>Performance, Security, and Operational Efficiency</strong></h4><ul>  <li><strong>Performance:</strong> Hosted on a CDN for fast load times.</li>  <li><strong>Security:</strong> No backend = minimal attack surface.</li>  <li>    <strong>Maintenance:</strong> No server or database to maintain,    dramatically lowering costs.  </li></ul><h4><strong>Limitations and Trade-offs</strong></h4><p>  Product data and content updates must be made directly in the HTML files,  which is manageable at small scale but not ideal for frequent inventory  updates.</p><h4><strong>Technology Stack</strong></h4><ul>  <li><strong>HTML5, CSS:</strong> For structure and responsive design.</li>  <li>    <strong>JavaScript:</strong> For cart, filtering, and checkout integration.  </li>  <li>    <strong>Web Storage API (localStorage):</strong> To persist cart state    across sessions.  </li></ul><h4><strong>Client-Side State Management</strong></h4><p>  I implemented shopping cart persistence using <code>localStorage</code>. This  allows users to keep their cart data intact even after browser refreshes or  session changes.</p><h4><strong>WhatsApp Checkout – 'Conversational Commerce'</strong></h4><p>  I built a unique client-side checkout that uses the WhatsApp API. Upon  clicking 'Place Order':</p><ol>  <li>JavaScript compiles the cart into a formatted message.</li>  <li>    It URL-encodes the message and redirects the user to WhatsApp via a    <code>https://wa.me</code> link.  </li>  <li>    The business receives the message directly and confirms the order manually.  </li></ol><p>  I also included fallback instructions for users who face issues, instructing  them to send a screenshot manually. This solution completely removes the need  for server-side code, a database, or payment gateway.</p><h3><strong>Functional Analysis: The Customer Journey</strong></h3><h4><strong>Information Architecture</strong></h4><p>  I created a clean and logical layout with a persistent header and footer,  containing links to all major pages including Catalogue, FAQ, Testimonials,  and Delivery Charges. I placed instructional messages across the site to guide  users through the unfamiliar order process.</p><h4><strong>Product Discovery</strong></h4><ul>  <li>    <strong>Homepage modules:</strong> Feature categories, restocked items, and    offers.  </li>  <li>    <strong>Catalogue Filters:</strong> Users can filter plants by category,    sunlight, watering needs, and transit risk.  </li>  <li>    <strong>Modal View:</strong> Clicking a product name opens a modal with full    plant info—no page reload needed.  </li></ul><h4><strong>Product Data Schema</strong></h4><p>Each plant has structured metadata including:</p><ul>  <li>Product Name, ID, Category</li>  <li>Original and Discounted Price</li>  <li>Sunlight, Watering Needs, Transit Risk</li>  <li>Tags like 'On Offer' or 'Indoor Plant'</li></ul><h4><strong>Cart Functionality</strong></h4><ul>  <li>Add/Remove Items</li>  <li>Quantity Adjustments</li>  <li>Subtotal Calculation with note: '+ Delivery'</li>  <li>Persistent Cart using <code>localStorage</code></li></ul><h4><strong>User-Centric Features</strong></h4><p>  I included a Transit Risk filter to help users avoid fragile plants, aligning  with the business’s no-replacement policy for high-risk items. This proactive  UX reduces friction and builds customer trust.</p><h3><strong>Business Operations and Support</strong></h3><h4><strong>Static Support Pages</strong></h4><ul>  <li>    <strong>FAQ Page:</strong> Details policies for payments, shipping, and    replacements.  </li>  <li>    <strong>Contact Page:</strong> Lists WhatsApp, email, and social channels.  </li>  <li>    <strong>Care Page:</strong> Tips on plant maintenance with filtering options    for plant types.  </li>  <li>    <strong>Delivery Charges:</strong> A pricing matrix based on location and    quantity.  </li></ul><h4><strong>Promotions and Incentives</strong></h4><p>  Orders over ₹1000 receive a free complementary plant—highlighted on the site  to encourage higher order values.</p><h4><strong>Site as Business Manual</strong></h4><p>  The entire site doubles as a transparent, always-available operational manual  for the business.</p><h3><strong>Strategic Outlook and Future Enhancements</strong></h3><ul>  <li>    <strong>Headless CMS:</strong> To make product management user-friendly and    scalable.  </li>  <li>    <strong>WhatsApp Business API:</strong> For automation of confirmations,    chatbots, and logging orders.  </li>  <li>    <strong>Client-Side Search:</strong> To improve product discoverability with    a library like Lunr.js.  </li>  <li>    <strong>Serverless Logging:</strong> Use Netlify or Firebase to back up    orders before redirection to WhatsApp.  </li></ul><h3><strong>Conclusion</strong></h3><p>  This project is a complete, real-world example of intelligent design under  tight constraints. It demonstrates my full-stack problem-solving ability, my  proficiency in static web development, and my creative use of existing tools  (like WhatsApp) to solve business challenges. RosaryPlantHouse.com isn’t just  a functional store—it’s a strategic, user-centered digital experience that I  crafted with care and purpose.</p>",
        "technology": "HTML, CSS, JavaScript, Ecommerce",
        "video": "",
        "image1": "Spikora1",
        "image2": "Spikora2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img12.png",
        "projectDate": "7 March 2022",
        "duration": "2 weeks",
        "github": "None",
        "App_link": "https://www.rosaryplanthouse.com"
      }
    },
    {
      "sys": {
        "id": "3"
      },
      "fields": {
        "name": "SubMerge",
        "category": "App",
        "competition": "SIH (Software edition)",
        "project": "Exploring the Aquatic Wonders - A Virtual Reality Underwater Museum",
        "description": "<h1>VR Underwater Experience - Project Documentation</h1><h2>Overview</h2><p>  I developed an immersive Virtual Reality (VR) application designed for Android  devices using Unity and Google VR. This application provides users with an  interactive underwater experience, enabling them to explore four unique  environments: Tower, Ship, Dance, and Ride. The user interacts with the  virtual world through intuitive gaze-based navigation.</p><h2>Main Menu and Navigation</h2><p>  Upon launching the application, the Unity logo is displayed, signaling the  underlying game engine. After loading, the user is greeted with a main menu  offering the following options:</p><ul>  <li>Tower</li>  <li>Dance</li>  <li>Ship</li>  <li>Ride</li></ul><p>  The menu is controlled using gaze-based input. Users simply look at one of the  menu options to make a selection.</p><h2>Scenes and Features</h2><h3>Tower Section</h3><p>  Selecting the 'Tower' option transports the user to a serene underwater  environment situated on the seabed. The scene includes:</p><ul>  <li>Blue sharks swimming across the scene.</li>  <li>A central tower-like structure providing architectural depth.</li>  <li>Pink and light blue aquatic creatures adding vibrant marine life.</li>  <li>Realistic seabed details such as rocks, seaweed, and marine flora.</li>  <li>Darker colored sharks and white whales gliding through the water.</li>  <li>    A glowing green orb positioned near the ocean floor to create intrigue.  </li></ul><h3>Ship Section</h3><p>  The 'Ship' environment features a large, partially submerged ship, viewed from  various angles as the camera pans dynamically. The surroundings include:</p><ul>  <li>Rocky ocean floor and background elements.</li>  <li>Colorful aquatic life, including blue and pink sea creatures.</li>  <li>Presence of black and white whales swimming nearby.</li></ul><h3>Dance Section</h3><p>  In the 'Dance' section, I created a choreographed sequence where marine  animals move in synchronization, resembling a dance performance. The animated  scene showcases:</p><ul>  <li>Light blue whales moving rhythmically.</li>  <li>Green aquatic creatures joining the formation.</li>  <li>Orcas (black and white) executing synchronized movements.</li>  <li>Other various marine species participating in the dance.</li></ul><h3>Ride Section</h3><p>  Choosing 'Ride' offers users a guided boat-like journey through the underwater  world. The ride interface includes basic 'Play' and 'Pause' functionality.  During the ride, users observe:</p><ul>  <li>    Large reddish-brown octopuses with long tentacles gracefully moving through    the water.  </li>  <li>Numerous colorful aquatic animals surrounding the user.</li>  <li>    Varied underwater perspectives as the camera glides through the marine    environment.  </li></ul><h2>Technology Stack</h2><ul>  <li>    <strong>Unity:</strong> Game engine used for developing the VR experience    and managing scenes, lighting, animations, and interactions.  </li>  <li>    <strong>Google VR SDK:</strong> Integrated for enabling VR support and    gaze-based input on Android devices.  </li>  <li>    <strong>Blender:</strong> Used to model and animate 3D marine life such as    sharks, whales, and octopuses.  </li>  <li>    <strong>Android:</strong> Target platform for deployment, tested with Google    Cardboard and similar devices.  </li>  <li>    <strong>C#:</strong> Programming language used within Unity for scripting    interactions and scene transitions.  </li></ul><h2>Conclusion</h2><p>  This project demonstrates the potential of immersive environments for  educational or entertainment purposes using affordable VR tools. It combines  realistic underwater visuals with intuitive navigation, resulting in a highly  engaging user experience.</p>",
        "technology": "Unity, Android App Dev, Virtual Reality",
        "video": "https://www.youtube.com/embed/9ubWJePQCwg",
        "image1": "SubMerge1",
        "image2": "SubMerge2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img1.jpg",
        "projectDate": "6 March 2022",
        "duration": "2 weeks",
        "github": "None",
        "App_link": "https://drive.google.com/file/d/1SkCRXHEn5xs_H8E9z1KcGtNdg5EASb57/view?usp=drive_link"
      }
    },
    {
      "sys": {
        "id": "4"
      },
      "fields": {
        "name": "ARcadium",
        "category": "App",
        "competition": "NSTF Codissia",
        "project": "IOTAR Application",
        "description": "<p align=\"justify\">&emsp;Augmented reality (AR) is a technology that overlays digital information such as sounds, videos, and graphics on top of the real-world environment. AR can enhance the learning experience by providing interactive and immersive content that can stimulate learners’ motivation, attention, and understanding. Internet of things (IoT) is a technology that connects physical devices, such as sensors, cameras, and actuators, to the internet and enables data transmission and synchronization. IoT can enable the collection and analysis of real-time data from the environment and support the creation of smart and adaptive learning scenarios.<br><br>\n\n    &emsp;IOTAR is a term that combines AR and IoT to create a novel learning paradigm that leverages the advantages of both technologies. IOTAR can provide learners with contextualized and personalized learning content that is based on their location, preferences, and performance. IOTAR can also enable learners to interact with both physical and virtual objects and manipulate them in real time.<br><br>\n\n<b>Our application is developed using AR for employee and student learning using IOTAR. Our application has the following features and benefits:</b>\n<ul>\n<li>It uses AR to display 3D models of objects or concepts that are relevant to the learning objectives, such as anatomy, chemistry, physics, or engineering. Learners can explore the models from different angles and perspectives, zoom in and out, and rotate them. This can improve their spatial abilities and comprehension.</li>\n<li>It uses IoT to collect data from sensors or devices that are embedded in the environment or worn by the learners, such as temperature, humidity, heart rate, or brain activity. The application uses this data to adjust the difficulty level, feedback, or content of the learning activities according to the learners’ needs and preferences. This can enhance their engagement and satisfaction.</li>\n<li>It uses AR to create gamified learning experiences that can motivate and challenge learners. The application uses elements such as points, badges, leaderboards, or quests to reward learners for their achievements and progress. The application also uses AR to create immersive scenarios or stories that can stimulate learners’ curiosity and creativity.</li>\n<li>It uses IoT to enable collaboration and communication among learners who are located in different places. The application allows learners to share their AR experiences with others through live video or audio chat. The application also allows learners to work together on projects or tasks that involve both physical and virtual objects.</li>\n<li>It uses AR to create a magical feeling of a 3D object appearing on top of the physical world. The application makes use of advanced projection techniques that can make the virtual objects look realistic and seamless with the real environment. The application also makes use of haptic feedback that can make the learners feel the texture or shape of the virtual objects.</li>\n</ul><br>\n\n<b>Our application can be used for various purposes and contexts, such as:</b>\n<ul>\n<li>Employee training: Our application can help employees learn new skills or update their knowledge in a fun and engaging way. Our application can also help employees practice their skills in realistic simulations that mimic their work environment.</li>\n<li>Student education: Our application can help students learn complex or abstract concepts in a visual and interactive way. Our application can also help students develop their creativity and problem-solving skills by allowing them to create their own AR projects.</li>\n<li>Lifelong learning: Our application can help anyone who wants to learn something new or pursue their interests in a flexible and convenient way. Our application can also help anyone who wants to enhance their memory and cognitive abilities by providing them with stimulating AR experiences.</li>\n<li>We believe that our application can transform education and enhance learning by using AR and IoT technologies. We hope that our application can inspire learners of all ages and backgrounds to explore, discover, and create.</li>\n</ul>\n</p>",
        "technology": "Unity, Android App Dev, Augmented Reality",
        "video": "https://www.youtube.com/embed/KQdLGSpzPxI",
        "image1": "ARcadium1",
        "image2": "ARcadium2",
        "image3": "ARcadium3",
        "projectDate": "4 March 2022",
        "duration": "2 year",
        "github": "None",
        "App_link": "None"
      }
    },
    {
      "sys": {
        "id": "5"
      },
      "fields": {
        "name": "YogaVision",
        "category": "App",
        "competition": "SIH (Hardware edition)",
        "project": "Fabricated IOT based VR box with Application",
        "description": "<p align=\"justify\">&emsp;Yoga is an ancient practice that involves physical, mental, and spiritual disciplines that aim to harmonize the body, mind, and soul. Yoga can provide various benefits such as improving flexibility, strength, balance, posture, breathing, blood circulation, immunity, and mental health. However, practicing yoga can also be challenging, especially for beginners or people who have busy or stressful lifestyles. It can be hard to find a suitable place, time, or instructor to do yoga. It can also be difficult to maintain focus, motivation, and relaxation while doing yoga.<br><br>\n\n&emsp;To address these challenges, we have developed a VR device that is specially designed to enhance the yoga experience. Our VR device is not just a regular headset that immerses you in a virtual world. It is a smart and interactive device that adapts to your needs and preferences and guides you through your yoga journey. <br><br>\n\n<b>Our VR device has the following features:</b>\n<ul>\n<li>Lightweight and durable headset: Our headset is made of flexible material and cotton cushion padding that makes it comfortable and easy to wear. It does not cause any pressure or irritation on your face or head. It is also durable and resistant to wear and tear.</li>\n<li>Next-gen lenses: Our lenses offer a wide field of view with significantly reduced glare for crystal-clear HD optics. You can enjoy a realistic and vivid view of the virtual environment without any distortion or blurriness.</li>\n<li>LRPF filters, inter-pupillary distance adjuster: Our filters provide eye protection and drastically improve vision clarity by blocking harmful blue light and enhancing contrast and color. Our adjuster allows you to customize the distance between the lenses according to your eye shape and size. This ensures optimal comfort and clarity for your eyes.</li>\n<li>Inbuilt spatial audio: Our headset has integrated speakers that deliver high-quality sound that matches the virtual environment. You can hear the sounds of nature, music, or the voice of the virtual tutor from different directions and distances. This creates a sense of immersion and presence in the virtual world.</li>\n<li>Pulse rate monitoring: Our headset has a sensor that measures your pulse rate and temperature while you do yoga. This helps you monitor your health and wellness and adjust your pace and intensity accordingly.</li>\n<li>Controller-less VR: Our device does not require any additional devices, such as controllers or wires, to operate. You can interact with the virtual environment using only your head movements, eye gaze, voice commands, or gestures. This makes the experience more natural and intuitive.</li>\n<li>Interactive virtual tutor: Our device has a virtual tutor that assists you with your yoga experience. The tutor can recognize your voice and respond to your questions or requests. The tutor can also provide instructions, feedback, tips, or encouragement while you do yoga. The tutor can adapt to your skill level, goals, and preferences and customize the yoga sessions accordingly.</li>\n<li>Gaze interaction: Our device has a gaze interaction feature that allows you to move through the VR application with just your eyes. You can change the environment with just a gaze for a couple of seconds. For example, you can gaze at a mountain icon to switch to a mountain scenery or gaze at a beach icon to switch to a beach scenery.</li>\n<li>Gyroscopic walk: Our device has a gyroscopic walk feature that allows you to walk through the environment with just a 30-degree head tilt. You can explore the virtual world and experience a safe, total VR of 360 degrees.\n<li>3D virtual buttons: Our device has 3D virtual buttons that allow you to easily interact and control the VR environment and characters. You can use these buttons to pause, resume, skip, repeat, or exit the yoga session. You can also use these buttons to select different options or settings for the yoga session.\n<li>Simple UI: Our device has a simple and user-friendly interface that ensures a smooth and hassle-free experience for all ages. You can easily navigate through the menus and options using your gaze or voice.\n</ul>\n<b>Our VR device can be used for various purposes and contexts, such as:</b>\n<ul>\n<li>Employee wellness: Our VR device can help employees improve their physical and mental health by providing them with an accessible and enjoyable way to do yoga. Our VR device can also help employees reduce stress, anxiety, fatigue, and burnout by providing them with a relaxing and refreshing escape from their work environment.</li>\n<li>Student education: Our VR device can help students learn about yoga and its benefits by providing them with an interactive and immersive way to practice yoga. Our VR device can also help students develop their concentration, memory, creativity, and problem-solving skills by providing them with stimulating and challenging VR experiences.</li>\n<li>Lifelong learning: Our VR device can help anyone who wants to learn or improve their yoga skills by providing them with a personalized and adaptive way to do yoga. Our VR device can also help anyone who wants to enhance their well-being and happiness by providing them with a fun and rewarding VR experience.</li>\n<li>We believe that our VR device can revolutionize yoga and enhance learning by using innovative features and technologies. We hope that our VR device can inspire learners of all ages and backgrounds to practice yoga and achieve their goals.</li>\n</ul>\n</p>",
        "technology": "Unity, Android App Dev, Virtual Reality",
        "video": "https://www.youtube.com/embed/F9xCMl1qXHo",
        "image1": "YogaVision1",
        "image2": "YogaVision2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img123.png",
        "projectDate": "5 March 2022",
        "duration": "5 year",
        "github": "https://github.com/sshibinthomass/yoga-SIH",
        "App_link": "https://drive.google.com/file/d/15zK17vOceYkonCtIoKgEE7D6U-fEYSnT/view?usp=drive_link"
      }
    },
    {
      "sys": {
        "id": "6"
      },
      "fields": {
        "name": "NaviBot",
        "category": "Robotics",
        "competition": "College Final Project",
        "project": "Navigation Bot",
        "description": "<h3>Funding & Recognition</h3><p>  This project was officially  <strong>funded under the Student Projects Scheme (SPS)</strong> by the  <strong>Tamil Nadu State Council for Science and Technology (TANSCST)</strong  >, an initiative under the Government of Tamil Nadu, during the academic year  2021–2022.</p><p>  The <strong>TANSCST Student Projects Scheme</strong> is a competitive funding  program that supports innovative and socially impactful final-year student  projects in science, engineering, and technology. Our project, titled  <em>'Design and Fabrication of DRO-CO Bot'</em>, was selected after a rigorous  evaluation of its technical viability, innovation, societal relevance, and  feasibility.</p><p>  The financial grant provided by TANSCST was utilized to design, prototype, and  fabricate an AI-powered humanoid robot equipped with functionalities such as  contactless temperature sensing, face mask detection, real-time face  recognition for attendance, voice-based AI interaction, and autonomous SOP  compliance enforcement.</p><p>  This recognition not only validated the technical and social importance of the  DRO-CO Bot but also helped us transform our concept into a fully working  prototype tested in an industrial environment. Being funded by TANSCST  significantly enhanced the visibility and credibility of our project at the  state level.</p><p>  The successful execution of this government-supported initiative highlights my  ability to independently develop, manage, and deliver impactful engineering  solutions under real-world constraints.</p><h3>Problem Statement</h3><p>  In response to the COVID-19 pandemic, there was an urgent requirement for  contactless systems to enforce Standard Operating Procedures (SOPs) like  temperature checks, mask compliance, and attendance in public environments.  Manual enforcement was risky and inefficient. The challenge was to build a  cost-effective, smart, and autonomous robot to assist in enforcing safety  rules and interact with users without human intervention.</p><h3>Project Summary</h3><p>  I conceptualized, designed, programmed, and fabricated a humanoid robot named  <strong>DRO-CO Bot</strong>, an AI-powered, IoT-integrated robotic assistant.  It can detect face masks, measure temperature without contact, recognize faces  for automatic attendance, and interact with users using speech recognition and  AI-driven responses. This system reduces human labor, enforces COVID-19 SOPs,  and serves as a personal assistant in offices, public spaces, and educational  institutions.</p><h3>Project Objectives</h3><ul>  <li>Design a humanoid robot integrated with AI and IoT functionalities</li>  <li>Implement real-time mask detection and face recognition</li>  <li>    Enable contactless temperature sensing and automatic attendance marking  </li>  <li>Integrate speech-based AI assistant functionalities using APIs</li>  <li>Develop a cost-effective, scalable, and safe system for public use</li></ul><h3>Features and Capabilities</h3><ul>  <li>Face mask detection using TensorFlow and MobileNetV2</li>  <li>Real-time face recognition using OpenCV and face_recognition library</li>  <li>Contactless temperature sensing using MLX90614 IR sensor</li>  <li>    Speech recognition and voice response using Python, pyttsx3, and Google    Speech API  </li>  <li>    AI-powered assistant with Wikipedia, WolframAlpha, and OpenWeather    integrations  </li>  <li>WhatsApp automation via Selenium for live reporting and alerts</li>  <li>    Servo motor-based robotic arm with 3 DOF controlled by Arduino (ATmega2560)  </li>  <li>Ultrasonic-based object detection and crowd monitoring</li>  <li>    Real-time attendance and temperature data logging using CSV and dashboard  </li></ul><h3>Technical Implementation</h3><ul>  <li>    Developed both <strong>forward and inverse kinematics</strong> for the    robotic arm using D-H parameters  </li>  <li>    Calculated <strong>torque, stress, strain, buckling load</strong> for robot    joints using PLA structural material  </li>  <li>    Designed and 3D printed all mechanical components using Fusion 360 and PLA    filament  </li>  <li>    Established I2C communication between Raspberry Pi and Arduino for sensor    control  </li>  <li>    Used <strong>servo motors (MG996R, RDS3115MG)</strong> for joint control    with PID-based position feedback  </li>  <li>    Set up Raspberry Pi as the master node for camera, display, temperature, and    voice processing  </li>  <li>    Built Python modules for face encoding, recognition, and voice-to-text    interaction  </li>  <li>    Integrated audio-visual output via 5” HDMI LCD display and Bluetooth speaker  </li></ul><h3>Engineering Design & Analysis</h3><ul>  <li>Degree of Freedom (DOF): 3 for arm, 1 for head</li>  <li>Forward Kinematics Equations for X and Y-plane positioning of the arm</li>  <li>    Inverse Kinematics based on end-effector coordinates using trigonometric    relations  </li>  <li>Torque calculations for each joint using dynamic and static loads</li>  <li>    Stress-Strain Analysis using Young’s Modulus and Safety Factor for PLA  </li>  <li>Euler Buckling check for column rigidity to ensure structural safety</li>  <li>    Bending Moment and Shear Force analysis for robotic arm and shoulder    stability  </li></ul><h3>Outcome</h3><ul>  <li>    Successfully fabricated and tested a fully functional autonomous COVID-19    SOP enforcement robot  </li>  <li>Recognized faces and recorded attendance with >95% accuracy</li>  <li>Detected mask violations in real-time and notified users via WhatsApp</li>  <li>    Voice assistant answered queries, fetched weather, and gave system responses  </li>  <li>    Achieved smooth and accurate robotic arm motion with no structural failure  </li>  <li>Tested successfully in a live industrial environment</li></ul><h3>Project Budget</h3><p>  Total Cost: ₹28,601 INR (Optimized through open-source tools and affordable  components)</p><h3>Future Enhancements</h3><ul>  <li>Extend surveillance to entire room using multiple camera feeds</li>  <li>BLE-based contact tracing and interaction logging</li>  <li>Solar power integration for sustainable off-grid operation</li>  <li>Multilingual and emotion-aware speech assistant</li>  <li>Human-following mode using real-time person tracking and mobile base</li></ul><h3>Technologies Used</h3><ul>  <li>    <strong>Hardware:</strong> Raspberry Pi 4, ATMEGA 2560, MLX90614, Ultrasonic    Sensors, Servo Motors  </li>  <li><strong>Programming:</strong> Python, C++, Arduino IDE</li>  <li>    <strong>Libraries:</strong> OpenCV, TensorFlow, face_recognition, Selenium,    pyttsx3, Wikipedia, WolframAlpha API  </li>  <li><strong>CAD & Fabrication:</strong> Fusion 360, PLA 3D Printing</li>  <li><strong>Protocols:</strong> I2C, GPIO, HDMI, USB, Bluetooth</li>  <li>    <strong>Others:</strong> CSV data logging, WhatsApp Web automation, TFT    display interface  </li></ul>",
        "technology": "Deep Learning, Machine Learning, Flask, Robotics, Django",
        "video": "https://www.youtube.com/embed/QqrCg2vfLW0",
        "image1": "NaviBot1",
        "image2": "NaviBot2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img12.png",
        "projectDate": "1 March 2022",
        "duration": "1 Year",
        "github": "None",
        "App_link": "None"
      }
    },
    {
      "sys": {
        "id": "7"
      },
      "fields": {
        "name": "LLM-Agent for eCADSTAR",
        "category": "LLM",
        "competition": "TU Dortmund Group Project",
        "project": "LLM-Agent for eCADSTAR",
        "description": "<h4>Introduction & Motivation</h4><p>  We set out to build a next-generation AI chatbot to support engineers with PCB  design tasks. The goal was to combine natural language processing, vision  models, simulation APIs, and information retrieval. Traditional LLMs were  limited to static text and prone to hallucination; our aim was to build a  multimodal, tool-augmented system capable of handling complex tasks in signal  integrity (SI), power integrity (PI), and electromagnetic compatibility (EMC).</p><h4>Vision-Language Models (Image Descriptor)</h4><p>  I evaluated and fine-tuned multiple models for interpreting circuit schematics  and simulation graphs:</p><ul>  <li>    <strong>Qwen2.5-VL:</strong> Best performer overall. High accuracy and    detailed recognition of components, SI/FD/TDR graphs.  </li>  <li>    <strong>Granite-Vision-3.2:</strong> Strong contender for both circuit and    waveform images, with excellent shape-based recognition.  </li>  <li>    <strong>SmolVLM & 500M:</strong> Lightweight but shallow outputs and    frequent hallucinations.  </li>  <li>    <strong>LLaVA-1.6-Mistral:</strong> Incorrect topologies and severe    hallucinations. Not suitable for engineering images.  </li></ul><p>  Each model was fine-tuned using LoRA adapters and 4-bit quantization to run  within 48GB GPU limits.</p><h4>Dataset Preparation</h4><ul>  <li>    Captured 4500+ annotated samples from eCADSTAR software (circuit schematics    + SI/FD/TDR waveform images).  </li>  <li>    Augmented dataset with component-only images to improve object recognition.  </li>  <li>    Used <strong>Albumentations</strong> for image augmentation (rotation,    flips, emboss, brightness).  </li>  <li>    Used <strong>Gemini and Qwen2.5</strong> for text augmentation    (paraphrasing, synonym replacement).  </li>  <li>    Data formatted in JSON format to support image-query-answer format for VLM    fine-tuning.  </li></ul><h4>Fine-Tuning Process</h4><ul>  <li>    Implemented Low-Rank Adaptation (LoRA) and quantization to fit models within    VRAM constraints.  </li>  <li>    Used a template with system messages prompting models to identify topology,    components, waveform behavior.  </li>  <li>    Used 20% of the dataset for evaluation with loss/accuracy tracked using    TensorBoard.  </li>  <li>    Final results:    <ul>      <li>        <strong>Qwen2.5-VL</strong> and <strong>Granite-Vision-3.2</strong>:        Best loss (&lt;3) and mean token accuracy (&gt;85%).      </li>      <li><strong>LLaVA</strong>: Very high loss and poor generalization.</li>    </ul>  </li></ul><h4>RAG (Retrieval-Augmented Generation)</h4><ul>  <li>    <strong>LangChain:</strong> Used as the orchestration backbone connecting    tools and LLMs.  </li>  <li>    <strong>ChromaDB:</strong> Vector store for all file-based and web-based    text chunks.  </li>  <li>    <strong>Embedding Models:</strong> Used Nomic-Embed-Text and    GPT4AllEmbeddings.  </li>  <li><strong>File Types:</strong> Supported PDF, DOCX, XLSX, CSV, PPTX.</li>  <li>    <strong>Web Retrieval:</strong> Used DuckDuckGo, Bing, Google, and URL    scraping for real-time knowledge access.  </li>  <li>    <strong>Hybrid Retrieval:</strong> Combined web + file content to build    enriched prompts for LLMs.  </li>  <li>    Performance: Sub-second retrieval even at 100K vectors; integrated into the    chatbot seamlessly.  </li></ul><h4>AI Agent & Orchestrator System</h4><ul>  <li>    <strong>LangChain’s React Agent</strong> design: Used structured reasoning +    tool calling steps.  </li>  <li>    <strong>Agent Inputs:</strong> Handled image-only, query-only, and image +    query combinations.  </li>  <li>    <strong>Memory:</strong> Short-term via LangChain Buffer, Long-term via    persistent vector DB history.  </li>  <li>    <strong>Tools Integrated:</strong>    <ul>      <li><strong>PCB Expert:</strong> LLaMA 8B model for PCB queries.</li>      <li>        <strong>Simulation API:</strong> Triggered eCADSTAR via PyAutoGUI for        SI, FD, TDR runs.      </li>      <li>        <strong>RAG Engine:</strong> Provided external document/web context when        needed.      </li>    </ul>  </li></ul><h4>Final Outcomes</h4><ul>  <li>Developed a fully integrated multi-modal chatbot for PCB engineers.</li>  <li>    Best performing model: <strong>Granite-Vision-3.2</strong> for circuit +    waveform images.  </li>  <li>    Robust RAG + Simulation toolchain enabled fact-based and analytical    reasoning.  </li>  <li>    System now supports simulation requests, layout validation, and real-time    Q&A via web search or file analysis.  </li></ul>",
        "technology": "LLM, RAG, Fine-Tuning, Langchain, Flask",
        "video": "",
        "image1": "eCADSTAR1",
        "image2": "NaviBot2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img12.png",
        "projectDate": "Sep 2024-April 2025",
        "duration": "8 months",
        "github": "None",
        "App_link": "None"
      }
    },
    {
      "sys": {
        "id": "8"
      },
      "fields": {
        "name": "Munich Sushi Chatbot",
        "category": "LLM",
        "competition": "Hobby Project",
        "project": "Munich Sushi Chatbot",
        "description": "  <h1>Munich Sushi Chatbot &ndash; Project Overview</h1>  <h2>Introduction</h2>  <p>    The Munich Sushi Chatbot is an AI-powered agent designed to assist users in    discovering sushi restaurants in Munich and finding suitable parking options    nearby. The system leverages advanced language models, real-time data    integration, and a modular architecture to provide up-to-date, context-aware    recommendations, reviews, and logistical information.  </p>  <hr />  <h2>Features</h2>  <h3>1. Sushi Restaurant Discovery</h3>  <ul>    <li>      <strong>Restaurant Listings:</strong> Provides a curated list of sushi      restaurants in Munich, including names, addresses, cuisine types, and      price levels.    </li>    <li>      <strong>Menu Information:</strong> Displays detailed menu items and prices      for each restaurant.    </li>    <li>      <strong>Contact Details:</strong> Supplies phone numbers, emails, and      website links for direct communication.    </li>    <li>      <strong>Google Reviews:</strong> Fetches and summarizes the latest Google      reviews for each restaurant.    </li>    <li>      <strong>Nearby Restaurants:</strong> Suggests alternative or similar      restaurants within a specified radius.    </li>  </ul>  <h3>2. Parking Assistance</h3>  <ul>    <li>      <strong>Parking Availability:</strong> Shows real-time availability of      parking spots near selected restaurants.    </li>    <li>      <strong>Open Parking Lots:</strong> Lists currently open parking      facilities, including distance, price, and free spot count.    </li>    <li>      <strong>24-Hour Parking:</strong> Identifies parking lots open around the      clock.    </li>    <li>      <strong>Disabled Access:</strong> Highlights parking options with disabled      access.    </li>    <li>      <strong>Payment Methods:</strong> Details accepted payment methods for      each parking lot.    </li>  </ul>  <h3>3. Additional Contextual Information</h3>  <ul>    <li>      <strong>Weather Integration:</strong> Provides current weather conditions      at or near the restaurant location.    </li>    <li>      <strong>User-Friendly Chatbot:</strong> Engages in natural conversation,      answering queries about restaurants, parking, and related topics.    </li>  </ul>  <hr />  <h2>Technology Stack</h2>  <h3>1. Programming Language</h3>  <ul>    <li>      <strong>Python 3.10+</strong>: The core application and all supporting      scripts are written in Python.    </li>  </ul>  <h3>2. Frameworks & Libraries</h3>  <ul>    <li>      <strong>LangGraph:</strong> For agentic workflow orchestration and state      management.    </li>    <li>      <strong>Streamlit:</strong> For building the interactive user interface.    </li>    <li>      <strong>FastMCP:</strong> For modular tool server/client communication      (Multi-Component Protocol).    </li>    <li>      <strong>OpenAI, Groq, Gemini, Ollama:</strong> For integrating various      LLMs (Large Language Models).    </li>    <li>      <strong>Google Maps API:</strong> For restaurant and parking location      data, reviews, and place details.    </li>    <li><strong>Open-Meteo API:</strong> For real-time weather data.</li>    <li><strong>dotenv:</strong> For environment variable management.</li>    <li><strong>Requests:</strong> For HTTP requests to external APIs.</li>    <li><strong>JSON:</strong> For structured data storage and retrieval.</li>  </ul>  <h3>3. Data Sources</h3>  <ul>    <li>      <strong>Local JSON Files:</strong> <code>data/sushi.json</code> and      <code>data/parking.json</code> store restaurant and parking data,      respectively.    </li>    <li>      <strong>Google Maps API:</strong> For live reviews and place details.    </li>    <li><strong>Open-Meteo API:</strong> For weather updates.</li>  </ul>  <hr />  <h2>Architecture & Implementation</h2>  <h3>1. Modular Tool Servers (MCP)</h3>  <ul>    <li>      <code>mcp_parking.py</code>: Exposes parking-related tools via FastMCP on      port 8003.    </li>    <li>      <code>mcp_restaurant.py</code>: Exposes restaurant-related tools via      FastMCP on port 8002.    </li>    <li>      <code>mcp_tools.py</code>: Aggregates tools for restaurant, parking,      weather, and reviews for unified access.    </li>  </ul>  <h3>2. Orchestration & Agent Logic</h3>  <ul>    <li>      <strong>Orchestrator:</strong> Coordinates tool usage and LLM calls to      generate structured, context-aware responses and reports.    </li>    <li>      <strong>Nodes:</strong> Specialized classes (e.g.,      <code>RestaurantRecommendationNode</code>) process user queries and      generate relevant responses.    </li>    <li>      <strong>MultiServerMCPClient:</strong> Connects to multiple MCP tool      servers to access restaurant and parking data seamlessly.    </li>  </ul>  <h3>3. Data Flow</h3>  <p>    <strong>User Query</strong> → <strong>UI (Streamlit)</strong> →    <strong>Agent Node</strong> → <strong>MCP Tool Server</strong> →    <strong>Data Source/API</strong> → <strong>Response</strong>  </p>  <p>    Uses RAG (Retrieval-Augmented Generation) to combine LLM output with    up-to-date data from local files and APIs.  </p>  <h3>4. User Interface</h3>  <ul>    <li>      <strong>Streamlit App:</strong> Provides an interactive web interface for      users to chat with the agent and receive live recommendations.    </li>    <li>      <strong>Configurable UI:</strong> Settings and appearance managed via      <code>uiconfigfile.ini</code> and <code>uiconfigfile.py</code>.    </li>  </ul>  <hr />  <h2>Graphs and Nodes: Workflow Orchestration</h2>  <p>    The application uses LangGraph to model its conversational and    data-processing workflow as a directed graph for modular orchestration.  </p>  <h3>Graph Structure</h3>  <ul>    <li>      <strong>StateGraph:</strong> Defines the flow of data and control between      nodes.    </li>    <li>      <strong>Start and End Nodes:</strong> Entry (<code>START</code>) and exit      (<code>END</code>) points for each graph.    </li>    <li>      <strong>Conditional Edges:</strong> Enables evaluation and fallback logic      for dynamic routing.    </li>  </ul>  <h3>Node Types</h3>  <ul>    <li>      <strong>BasicChatbotNode:</strong> Handles general conversation and      queries using LLMs.    </li>    <li>      <strong>RestaurantRecommendationNode:</strong> Manages restaurant-related      queries using RAG and tool access.    </li>    <li>      <strong>Evaluation Node:</strong> Assesses quality of response and decides      on further actions.    </li>    <li>      <strong>Store Node:</strong> Handles memory storage with deduplication and      relevance filtering.    </li>    <li>      <strong>Search Node:</strong> Executes additional search logic if      required.    </li>    <li>      <strong>Orchestrator Node:</strong> Plans and coordinates structured      report generation.    </li>    <li><strong>LLM Call Node:</strong> Makes actual LLM API calls.</li>    <li>      <strong>Synthesizer Node:</strong> Aggregates multiple node outputs into a      final result.    </li>    <li>      <strong>Export Markdown Node:</strong> Exports final output to markdown      format.    </li>  </ul>  <h3>Example Graph Workflows</h3>  <ul>    <li>      <strong>Basic Chatbot Graph:</strong> <code>START → chatbot → END</code>    </li>    <li>      <strong>Restaurant Recommendation Graph:</strong>      <code>START → restaurant_node → END</code>    </li>    <li>      <strong>Assistant Chatbot Graph:</strong>      <code        >START → chatbot → evaluate_node → (store_node or search_node) →        store_node → END</code      >    </li>    <li>      <strong>Report Generation Graph:</strong>      <code        >START → orchestrator → llm_call (parallel) → synthesizer →        export_markdown → END</code      >    </li>  </ul>  <h3>Graph Building and Extensibility</h3>  <ul>    <li>      <strong>GraphBuilder:</strong> Dynamically constructs graphs for specific      use cases.    </li>    <li>      <strong>Modular Design:</strong> Nodes and workflows are easily      extendable.    </li>    <li>      <strong>Conditional Logic:</strong> Supports intelligent routing and      fallback within graphs.    </li>  </ul>  <h3>Benefits</h3>  <ul>    <li><strong>Modularity:</strong> Easy maintenance and future upgrades.</li>    <li>      <strong>Parallelism:</strong> Report generation runs concurrently for      efficiency.    </li>    <li>      <strong>Dynamic Routing:</strong> Enables adaptive conversation flow.    </li>  </ul>  <hr />  <h2>Example Use Case</h2>  <p>    <strong>User:</strong> 'Find me a sushi restaurant near Marienplatz with    good reviews and available parking.'  </p>  <p><strong>Agent:</strong></p>  <ul>    <li>Searches for sushi restaurants near Marienplatz.</li>    <li>Retrieves menus, contact info, and reviews.</li>    <li>Checks parking availability and proximity.</li>    <li>Returns a concise summary with all details.</li>  </ul>  <hr />  <h2>Extensibility</h2>  <ul>    <li>      <strong>Add New Tools:</strong> New MCP scripts can be created for      additional domains like hotels or events.    </li>    <li>      <strong>Swap LLMs:</strong> Easily integrate different language models      (OpenAI, Ollama, etc.).    </li>    <li>      <strong>Custom Data:</strong> Accepts new JSON files or APIs for different      data types.    </li>  </ul>  <hr />  <h2>Conclusion</h2>  <p>    The Munich Sushi Chatbot demonstrates a robust, modular approach to building    AI-powered assistants for location-based recommendations. Its architecture    allows for easy expansion, real-time data integration, and a seamless user    experience for anyone seeking sushi and parking in Munich.  </p>",
        "technology": "Open-AI, LLM, RAG, LangGraph, Streamlit, ChromaDB, MCP, Google Maps, Web RAG",
        "video": "https://www.youtube.com/embed/8AhdfVI-oU8?si=_zJ6zAP9A762ZIFO",
        "image1": "Munich-Sushi1",
        "image2": "NaviBot2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img12.png",
        "projectDate": "June 2025",
        "duration": "2 weeks",
        "github": "https://github.com/sshibinthomass/Munich-sushi-Chatbot",
        "App_link": "None"
      }
    },
    {
      "sys": {
        "id": "9"
      },
      "fields": {
        "name": "Langgraph Core",
        "category": "LLM",
        "competition": "Open Source Project",
        "project": "Langgraph Core",
        "description": "",
        "technology": "Open-AI, Groq, Gemini, Ollama, LLM, LangGraph, Streamlit",
        "video": "",
        "image1": "Langgraph-Core1",
        "image2": "NaviBot2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img12.png",
        "projectDate": "May 2025",
        "duration": "1 month",
        "github": "https://github.com/sshibinthomass/Langgraph-Core",
        "App_link": "None"
      }
    },
    {
      "sys": {
        "id": "10"
      },
      "fields": {
        "name": "Slug Flow Crystallization",
        "category": "Robotics",
        "competition": "MLME Project",
        "project": "Langgraph Core",
        "description": "<h1>Data-based Modeling of Slug Flow Crystallization with Uncertainty Quantification</h1><p><strong>Authors:</strong></p><ul>  <li>Shibin Thomas Stanley Paul (259782)</li>  <li>Aadhithya Krishnakumar (258741)</li>  <li>Nishitkumar Karkar (257529)</li>  <li>Sankar Santhosh Nair (258852)</li></ul><h2>Abstract</h2><p>This work presents a comprehensive machine learning framework for modeling slug flow crystallization processes with uncertainty quantification. The approach combines unsupervised clustering to identify distinct crystallization regimes, NARX neural networks for dynamic behavior prediction, and Conformalized Quantile Regression (CQR) for robust uncertainty estimation. The complete framework demonstrates effective uncertainty quantification for pharmaceutical crystallization processes, enabling robust process control applications.</p><h2>I. Introduction</h2><p>Slug flow crystallization (SFC) is critical in pharmaceutical manufacturing where control over particle size distribution impacts product quality. This report presents a machine learning methodology combining clustering, NARX neural networks, and CQR to model the crystallization process and quantify prediction uncertainty.</p><h2>II. Data Analysis and Clustering</h2><h3>A. Data Preprocessing and Quality Assessment</h3><p>The dataset initially included 98 files; 7 with extreme values were removed. The remaining 91 files were cleaned using custom bounds and the IQR method, with IQR proving more robust. 73 files were used for training and 18 for validation.</p><h3>B. Clustering Analysis and Regime Identification</h3><p>Statistical features were extracted to form 54-dimensional vectors, normalized, and clustered using K-means with k=2. Clustering metrics indicated good separation, and the clusters were balanced: 42 in Cluster 0, 31 in Cluster 1.</p><h2>III. NARX Model Development</h2><h3>A. Architecture and Training Strategy</h3><p>NARX models used deep feedforward networks with residual connections and 6 hidden layers. Each cluster had a dedicated model. Input sequences used a LAG of 40 steps. Feature scaling was applied per cluster.</p><h3>B. Training Configuration and Convergence</h3><p>Training used AdamW optimizer, weighted MSE loss, early stopping, and learning rate scheduling. Models converged well with minimal overfitting and strong tracking of process dynamics.</p><h2>IV. Uncertainty Quantification</h2><h3>A. Conformalized Quantile Regression Implementation</h3><p>CQR used neural networks with pinball loss for τ=0.1 and τ=0.9. 12 separate models were trained (6 variables × 2 quantiles). Training was robust with good convergence for all models.</p><h3>B. Conformal Calibration and Coverage Analysis</h3><p>Calibration used conformity scores to compute delta for adjusting quantile intervals. Resulting intervals achieved ~90% coverage across all variables, adapting to dynamics and measurement noise.</p><h3>C. Uncertainty Propagation Methods</h3><p>Three methods were implemented for uncertainty propagation: Gaussian Error Propagation, Monte Carlo Simulation, and Simplified Kalman Filter. These simulated real-world prediction with exogenous inputs after the lag window.</p><h2>V. Results</h2><p>Independent test evaluation showed strong performance across all variables. The integrated pipeline (clustering + NARX + CQR) achieved accurate predictions with uncertainty quantification suitable for process control.</p><h3>Performance Table</h3><table border="1">  <tr>    <th>State</th>    <th>MSE (Open Loop)</th>    <th>MAE (Open Loop)</th>    <th>MSE (Closed Loop)</th>    <th>MAE (Closed Loop)</th>  </tr>  <tr>    <td>c</td>    <td>4.91e-07</td>    <td>4.81e-04</td>    <td>1.40e-07</td>    <td>2.43e-04</td>  </tr>  <tr>    <td>T_PM</td>    <td>2.20e-01</td>    <td>2.93e-01</td>    <td>2.94e-02</td>    <td>1.26e-01</td>  </tr>  <tr>    <td>d50</td>    <td>8.59e-11</td>    <td>6.93e-06</td>    <td>5.69e-11</td>    <td>5.72e-06</td>  </tr>  <tr>    <td>d10</td>    <td>1.17e-10</td>    <td>8.16e-06</td>    <td>7.86e-11</td>    <td>6.69e-06</td>  </tr>  <tr>    <td>d90</td>    <td>2.11e-10</td>    <td>1.13e-05</td>    <td>1.86e-10</td>    <td>1.08e-05</td>  </tr>  <tr>    <td>T_TM</td>    <td>2.24e-01</td>    <td>2.61e-01</td>    <td>2.96e-02</td>    <td>1.14e-01</td>  </tr></table><h2>Acknowledgment</h2><p>The authors acknowledge the support of the Machine Learning Methods for Engineers course at TU Dortmund and the comprehensive dataset provided for this research.</p><h2>References</h2><ol>  <li>C. M. Bishop, Pattern Recognition and Machine Learning. Springer, 2006.</li>  <li>Y. Romano, E. Patterson, and E. J. Candès, "Conformalized quantile regression," arXiv:1905.03222, 2019.</li>  <li>M. Termühlen et al., "Continuous slug flow crystallization," Chem. Eng. Res. Des., vol. 170, pp. 290–303, 2021.</li>  <li>R. Koenker and G. Bassett Jr., "Regression quantiles," Econometrica, vol. 46, no. 1, pp. 33–50, 1978.</li>  <li>V. Vovk, A. Gammerman, and G. Shafer, Algorithmic Learning in a Random World. Springer, 2005.</li>  <li>J. Lei et al., "Distribution-free predictive inference for regression," J. Amer. Statist. Assoc., vol. 113, no. 523, pp. 1094–1111, 2018.</li></ol>",
        "technology": "NARX, CQR, Tensorflow, Data Preprocessing, Clustering, Time Series",
        "video": "",
        "image1": "Langgraph-Core1",
        "image2": "NaviBot2",
        "image3": "https://raw.githubusercontent.com/sshibinthomass/Rosary-Plant-House/master/assets/img/img12.png",
        "projectDate": "July 2025",
        "duration": "2 months",
        "github": "https://github.com/sshibinthomass/MLME_SFC_13",
        "App_link": "None"
      }
    }
  ]
}
